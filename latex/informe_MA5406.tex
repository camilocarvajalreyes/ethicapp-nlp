% CREACIÓN DEL DOCUMENTO
\documentclass[
	spanish, % Idioma: spanish, english, etc.
	letterpaper, oneside
]{article}

% INFORMACIÓN DEL DOCUMENTO
\def\documenttitle {\textbf{Informe Final MA5406} \\ Procesamiento de datos textuales EthicApp con algoritmos de procesamiento de lenguaje natural}
\def\documentsubtitle {}
\def\documentsubject {Tema a tratar}
\def\documentdate {21 de julio 2023 }

\def\documentauthor {Camilo Carvajal Reyes}
\def\coursename {Probabilidad y Estadística en el Análisis de Datos MA5406}
\def\coursecode {}

\def\universityname {Universidad de Chile}
\def\universityfaculty {Facultad de Ciencias Físicas y Matemáticas}
\def\universitydepartment {Departamento de Ingeniería Matemática}
\def\universitydepartmentimage {departamentos/logo_ethics3}
\def\universitylocation {Santiago de Chile}

% IMPORTACIÓN DEL TEMPLATE
\input{template}

% INICIO DE PÁGINAS
\begin{document}
	
% CONFIGURACIÓN DE PÁGINA Y ENCABEZADOS
\templatePagecfg


% CONFIGURACIONES FINALES
\templateFinalcfg

% ======================= INICIO DEL DOCUMENTO =======================

% Título y nombre del autor
\inserttitle

\begin{abstract}
    La aplicaión \textit{EthicApp} es una herramienta que permite recolectar preferencias de estudiantes ante dilemas éticos y sus justificaciones. Lamentablemente, el gran volumen de datos (del orden de dos mil textos) dificulta el análisis de estos. Este trabajo aborda el uso de modelos de aprendizaje de máquina supervisados y no-supervisados para modelar la estructura textual de las respuestas y con esto apoyar el análisis que puedan hacer los equipos docentes. Pese al potencial de mejora, la metodología propuesta ofrece una visión general de las respuestas y conceptos utilizados, lo que permitirá tomar decisiones informadas y justificadas para evaluar las competencias en éticas de estudiantes de la FCFM.
\end{abstract}


\section{Introducción}

\subsection{Descripción de la problemática}

En el marco de la enseñanza de la formación ética en la FCFM, se ha utilizado la aplicación EthicApp para la obtención y posterior análisis de las decisiones morales de estudiantes ante un dilema ético. Estos casos de estudio consisten en una problemática, que se plantea en forma de pregunta. Como respuesta a esta pregunta, los estudiantes manifiestan una preferencia en la forma de un número entre 1 y 6, donde los extremos de esta escala representan posturas dispares en cuanto a la decisión a tomar.

\insertimage[\label{img:respuesta1}]{img/intro2}{scale=0.75}{Estructura de respuestas de estudiantes.}

Luego de una primera respuesta se realiza una instancia de deliberación grupal, donde se toma una nueva decisión en conjunto. Finalmente, los estudiantes otorgan nuevamente una calificación, que puede diferir de la señalada en las dos instancias anteriores.

\insertimage[\label{img:respuesta2}]{img/intro4}{scale=0.75}{Iteraciones de las respuestas de estudiantes.}

\newp El Área de Ética de la FCFM ha llevado a cabo un completo análisis de las posturas de los estudiantes. No obstante, un aspecto difícil de procesar son las justificaciones que deben colocar luego de cada instancia de decisión. Pese a que análisis cualitativos de algunas respuestas han permitido plantear hipótesis preliminares respecto a los juicios morales y justificaciones de las decisiones, la gran magnitud de datos presentes dificultan la tarea de tomar conclusiones acerca de las competencias éticas exprimidas en la instancia, y como consecuencia la verificación de la adquisición de esta durante los cursos formativos de la escuela.


\subsection{Proposición de solución}

La ciencia de datos es una disciplina en constante crecimiento, en particular en nuestra facultad. En particular, los métodos basados en aprendizaje de máquinas han experimentado un aumento considerable en sus capacidades, ayudado por la mejora en capacidad de cómputo en las últimas décadas. Los algoritmos de procesamiento han sido una demostración de aquello, con modelos conversacionales como \textit{chatGPT} tomando protagonismo entre los medios y el público. 

\newp Dentro de esta rama se encuentran los modelos de lenguaje, que intentan aproximar modelar uno o más lenguajes humanos al inducir una probabilidad a secuencias de palabras (frases, oraciones o documentos). Estos modelos se implementan usando redes neuronales profundas y siendo entrenados en grandes volúmenes de datos textuales (córpuses). Finalmente, un modelo sirve para resolver variadas tareas de procesamiento de texto, incluyendo clasificación, pregunta-respuesta e categorización/identificación de elementos relevantes de una secuencia. Entre las desventajas que presentan estos modelos están su costo de entrenamiento y capacidad limitada de interpretabilidad, ambas consecuencias de su gran tamaño.

\newp Se propone la implementación de estos modelos pero también el uso de algoritmos más simples y más interpretables, para procesar los argumentos escritos por estudiantes en sus decisiones éticas. Más precisamente, se procederá a:

\begin{enumerate}
    \item Explorar las justificaciones textuales de las respuestas usando técnicas de minería de datos.

    \item Implementar modelos estadísticos para texto, que sean interpretables, para predecir la respuesta (número en la escala de 1 a 6) de los estudiantes utilizando el texto de justificación.

    \insertimage[\label{img:modelo1}]{img/modelo1}{scale=0.75}{Ejemplo de modelo para predecir posturas de estudiantes.}
    
    \item Identificar, a través del texto, elementos semánticos que justifiquen los argumentos dados por los estudiantes. Esto usando tanto el análisis exploratorio de datos como los algoritmos.
\end{enumerate}


\newp Si es que los modelos muestran una buena capacidad de predicción, se pueden usar como herramienta que a la larga servirá para evaluar la progresión de competencia ética de los estudiantes con menor inversión humana. Esto es de particular relevancia para los objetivos finales del área de ética. Por otro lado, existe un interés en verificar hasta que punto los algoritmos pueden modelar relaciones semánticas complejas como lo son las justificaciones morales de estudiantes ante a una problemática. Este es un objetivo complementario y que logrará plantear nuevas perspectivas de investigación cualquiera sea el resultado, tanto del punto de vista computacional como del estudio de la ética.

\newp Este proyecto consistirá de la utilización de diversas técnicas de modelamiento de lenguaje que permitan tomar una fotografía global de las justificaciones de estudiantes antes las preferencias escogidas.
De manera sistemática se considerarán los datos del caso Adela, a modo de no sobrecargar el informe de información. No obstante, los códigos para cada caso pueden encontrarse en el \href{https://github.com/camilocarvajalreyes/ethics-nlp}{repositorio de github del trabajo}. Se provee un resumen del caso Adela a continuación:

\newp \textit{En Chile, la deficiencia de vitamina D es un problema serio tanto en adultos mayores como en niños. Para abordar esta preocupante situación, un grupo de profesionales creó una startup que encontró una fruta ancestral de las comunidades diaguitas con alta concentración de vitamina D. Adela, una ingeniera del equipo, diseña el proceso de producción de un nuevo alimento a base de esta fruta. Sin embargo, se enfrenta a desafíos, ya que el árbol solo crece cerca de los ríos y necesita abundante luz solar, lo que dificulta llevarlo a zonas más australes afectadas por la sequía. Además, para conservar la vitamina D durante el transporte, el equipo decide liofilizar la fruta y agregar conservantes. Aunque aún no tienen la obligación legal de integrar a las comunidades diaguitas en el proyecto, Adela escucha sus preocupaciones sobre cómo estos cambios afectarían sus tradiciones. Aunque los cambios son necesarios para ayudar a quienes sufren deficiencia de vitamina D, las comunidades prefieren mantener sus prácticas tradicionales, ya que estas son parte fundamental de su identidad.}

Adela debería priorizar:
\begin{itemize}
    \item[(1)] Producir el alimento contra el déficit
    \item[(6)] Resguardar las tradiciones indígenas
\end{itemize}


\section{Estado del arte}

El procesamiento del lenguaje natural (NLP) ha ganado la atención de los medios y la sociedad en general en los últimos meses. Los últimos modelos conversacionales como chatGPT han generado mucha atención del público. A pesar de las capacidades que han demostrado, este tipo de modelos no son capaces de procesar ideas de la misma manera que lo hacen los humanos. Este es especialmente el caso de las decisiones que implican juicios morales.

% Responding to a question ethically has thus being studied in the context of large language models in order to improve their capability of assisting human beings. Moreover, when training models to predict human like responses to ethical problems and other general NLP tasks, we are sometimes able to detect patterns and structures that might explain how different cultures carry out morality.
\newp En este contexto, responder éticamente a una pregunta se ha estudiado en el contexto de grandes modelos de lenguaje para mejorar su capacidad de ayudar a los seres humanos. Además, cuando entrenamos modelos para predecir respuestas similares a las humanas a dilemas éticos y otras tareas generales de NLP, a veces somos capaces de detectar patrones y estructuras que podrían explicar cómo las diferentes culturas enfrentan problemas morales.

\newp Varios son los trabajos que buscan la predicción/identificación de elementos morales en texto natural. En esta sección abordaremos algunos de ellos. Primeramente, Garten et al. 2016 busca la detección automática de retóricas morales \cite{garten}. Para esto usan el diccionario de fundamentos morales \cite{tfd} (de la Teoría de fundamentos morales TMF \cite{tmf}), combinado con {word embeddings}. Otro trabajo similar es el de Xie et al. que evalua modelos en la clasificación de dilemas morales en los distintos fundamentos, gracias a lo cual concluyen que modelos de lenguaje tienen ventaja sobre modelos como las representaciones distribucionales \cite{xie}.


\newp Por otro lado, Kennedy et al. buscan la predicción de preocupaciones morales propias a un individuo usando evidencias de lenguaje moral escritas por este \cite{diff} (estados de Facebook de usuarios que hayan contestado el cuestionario de fundamentos morales). Se utilizaron distintas técnicas de procesamiento de lenguaje para predecir los puntajes obtenidos por los usuarios, para cada una de las dimensiones morales planteadas en la TMF \cite{tmf}. Se destacan la variedad de métodos para vectorizar texto testeados, incluyendo \textit{latent dirichlet allocation} (LDA) \cite{lda}, \textit{word embeddings}, conteo de ocurrencias del DFM \cite{tfd} y BERT \cite{bert}, que corresponde a un modelo de lenguaje profundo. Es este último que obtiene mejores resultados. Finalmente, tanto conteos de diccionario como LDA se usaron para interpretar que elementos linguísticos específicos explicaban cada fundamento por separado.


\newp Los trabajos mencionados en este reporte muestran la alta variedad de formatos en los cuales se ha evaluado la presencia y grado de categorías morales, así como también la capacidad de distintos modelos de procesamiento de lenguaje natural para modelarlos. No obstante, ninguno de los artículos estudiados enfrenta un desafío tan específico como el nuestro. Los datos que poseemos tienen la ventaja de poder verse desde distintos ángulos, lo cual plantea dificultades pero también abre puertas a que las conclusiones que se puedan tomar sean reflejo de nuevos descubrimientos en el área. Para finalizar, muchos de los artículos nos confirman la pertinencia de los modelos a usar, en el caso de modelos de lenguaje, y nos sugieren algunos tipos de modelos más simples que tengan capacidad interpretativa.


\section{Análisis exploratorio de datos}

Un análisis exploratorio adecuado se vuelve particularmente relevante a la hora de tomar conclusiones acerca de los datos textuales. Nos remitiremos al reporte de los largos de texto y $n$-gramas más frecuentes tanto de manera global como restringiéndonos a la etapa a evaluar o postura escogida por los estudiantes. A modo informativo se incluyen las frecuencias para cada uno de los casos en la tabla \ref{tab:freqs_casos}.

% \insertimagerightline[\label{figure:freq}]{img/tabla_2022.png}{0.5}{6}{Frecuencias por caso.}
\begin{table}[htbp]
\centering
\caption{Resultados de clasificación de posturas para caso Adela}
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Caso}  & \textbf{Cursos} & \textbf{Cantidad de estudiantes} & \textbf{Cantidad de grupos} \\ \hline
Julieta        & \textbf{1}      & \textit{819}                     & 247                         \\ \hline
\textbf{Adela} & \textbf{3}      & \textbf{1866}                    & \textbf{515}                \\ \hline
Laura          & 1               & 602                              & 335                         \\ \hline
Alicia         & 2               & 1628                             & 549                         \\ \hline
\end{tabular}
\label{tab:freqs_casos}
\end{table}

\begin{images}[\label{figure:tokens}]{Visualización de textos más frecuentes del dataset para el caso Adela}
    \addimage{adela/1_gramas_adela}{width=6.5cm}{1-gramas más frecuentes}
    \addimage{adela/3_gramas_adela}{width=8.5cm}{3-gramas más frecuentes}
    \imagesnewline
    \addimage{adela/nube_adela}{width=12cm}{Nube de palabras}
\end{images}

En la figura \ref{figure:tokens} se observan los $n$-gramas más frecuentes para $n \in \{ 1, 3 \}$. Se ignoraron los $2$-gramas pues en general eran concatenaciones de dos palabras que carecían de significado sin tener una tercera. En el caso de los $1$-gramas, i.e., palabras más frecuentes, destacan ``agua'', ``recursos'' y ``tradiciones'', que son elementos importantes en el platemiento del dilema.

\begin{images}[\label{figure:largos}]{Visualización de largos de texto para caso Adela, con y sin \textit{stop-words}.}
    \addimage{adela/hist_largo_adela}{width=7.5cm}{Histogramas de largo de texto}
    \addimage{adela/boxplot_largo_adela}{width=7.5cm}{Boxplot de largos de texto}
\end{images}

\insertimagerightline[\label{figure:freq_postura}]{img/adela/freq_adela.png}{0.65}{19}{Frecuencias de posturas por etapa en caso Adela}

En cuanto a los largos en general de los textos para el caso, estos son incluidos en la figura \ref{figure:largos}. Los largos de mensaje tienen un promedio de palabras inferior al $50\%$ para el caso Adela. También se incluye la cantidad de palabras que son relevantes dentro del texto. Para esto, se removieron las llamadas \textit{stop-words}, que son palabras que no aportan información al mensaje y solo contribuyen a la grámaica de este (algunos ejemplos incluyen ciertos artículos y conjunciones). 

\newp Las frecuencias de cada elección son mostradas en la figura \ref{figure:freq_postura}, donde se aprecia el hecho de que no hay un cambio brusco entre una etapa y otra, en especial entre las etapas individual 1 e individual 2. Recordemos que entre medio de esas existe una etapa grupal, para la cual se observa una mayor frecuencia de la postura (3), lo cual puede deberse al hecho de que estudiantes con posiciones similares están tratando de llegar a un consenso para colocar la valoración, lo cual por contraste disminuye la frecuencia de posiciones ``extremas'' como (1) y (6).

\subsection{Condicionamiento a selección de postura y etapa de actividad}

Una hipótesis a considerar en cuanto al rol del largo de palabras en la postura de estudiantes es que aquellas posiciones extremas tendrán una explicación más larga en extensión para explicarla. Por el contrario, se observa un efecto inverso, donde las posturas (1) y (6) tienen menor largo promedio que las preferencias más moderadas. Esto puede responder al hecho de que una persona absolutamente convencida de su postura vea menos necesidad de explicar esta. De cualquier modo la diferencia es sutil, por ende no se usará la variable implícita de largo de texto para las tareas de aprendizaje, pues se postula que aportará poca información.

\insertimage[\label{img:boxplot_largo_sel}]{img/adela/boxplot_largo_sel_adela}{scale=0.5}{Boxplots de largos de texto para cada elección de postura.}

\insertimagerightline[\label{figure:patterns}]{img/adela/1_gramas_sel_adela.png}{0.7}{16}{Frecuencias de posturas por etapa en caso Adela}

\newp Por otro lado, las palabras más frecuentes por cada postura se condicen con la justificación más comúnmente utilizada en cada caso. Se aprecia por ejemplo que para apoyar la producción del alimento se evocan conceptos como ``salud'' y ``personas'', asu como también acciones como ``producir'' y ``priorizar'', este último reflejando que estudiantes que asumen esa postura probablemente priorizan el bien común y/o salud de niños y ancianos en su decisión. En contraste, sustantivos como ``cultura'' y ``pueblos'' (en plural) aparecen al escoger resguardar las tradiciones. Así mismo, acciones como ``resguardar'', ``respetar'' y ``debería'' son usadas con el mismo propósito. Este simple análisis nos permite también identificar palabras ampliamente repetidas sin importar la postura del estudiante, como es el caso de ``tradiciones''.

\insertimage[\label{img:modelo3}]{img/adela/boxplot_largo_etapa_adela}{scale=0.5}{Boxplots de largos de texto para cada etapa de la actividad.}



\section{Metodología}

\subsection{Preprocesamiento}

Se procedió a eliminar palabras que no aportasen contenido a la justificación en cuestión. Esta poda de palabras incluyó las llamadas \textit{stop words}. Se consideró en un princió la \textit{stemmisation} de palabras. Esto corresponde a dejar la raiz de modo que palabras con distintas conjugaciones o modos pero que denotan lo mismo correspondan a la misma variable. Esto no mejoró las métricas, por lo cual procedemos a dejar las palabras tal cual. Además se eliminó la puntuación, en particular los paréntesis, que agregaban ruido especialmente para la modelación con tópicos. Estos elementos, sumados a la limpieza de ciertos elementos no informativos en el dataset contribuyeron a la sustancial mejora de las métricas mostradas en la presentación parcial, como se discute más adelante.

\subsection{Predicción supervisada de posturas con \textit{Naive-Bayes}}

Los métodos de Naive Bayes son un conjunto de algoritmos de aprendizaje supervisado basados en la aplicación del teorema de Bayes con la suposición ``naive'' de independencia condicional entre cada par de características dadas las clases variables. Consideramos vectores de $n$ dimensiones de ocurrencias introducidas anteriormente, cada uno correspondiente a un documento sobre un vocabulario de longitud $n$. Dado (un documento) $x=(x_1,\dots,x_n)$, nos gustaría etiquetarlo con una de las siguientes etiquetas: $C_1,\dots,C_k,\dots,C_K$. Gracias al teorema de Bayes, se tiene que:

$\forall k \in \{1,\dots,K\}$
%$$ \mathbb{P}(C_k|x_1,\dots,x_n)=\displaystyle\frac{\mathbb{P}(C_k)\mathbb{P}(x_1,\dots,x_n|C_k)}{\mathbb{P}(x_1,\dots,x_n)}$$
\begin{alignat*}{2}
    \mathbb{P}(C_k|x_1,\dots,x_n) & = \displaystyle\frac{\mathbb{P}(C_k)\mathbb{P}(x_1,\dots,x_n|C_k)}{\mathbb{P}(x_1,\dots,x_n)}\\
                                    & \propto \mathbb{P}(C_k)\mathbb{P}(x_1,\dots,x_n|C_k)
\end{alignat*}

El término $\mathbb{P}(C_k)$ se puede estimar con la frecuencia de la etiqueta $C_k$ en los datos. Para calcular la probabilidad de verosimilitud $\mathbb{P}(x_1,\dots,x_n|C_k)$, primero asumimos una distribución de probabilidad (como la Gaussiana o Multinomial). En el caso de clasificación binaria ($k\in{0,1}$) utilizando una distribución de Bernoulli, obtenemos, para cada característica $j$ (palabra):

$$ \mathbb{P}(x_j|C_k) = \mathbb{P}(j|C_k)x_j + (1-\mathbb{P}(j|C_k))(1-x_j)$$

$\mathbb{P}(j|C_k)$ también se puede estimar, tomando la proporción de documentos que contienen la palabra $j$ entre los de clase $C_k$. Este modelo permite la detección de spam y el análisis de sentimientos, y tiene un rendimiento aceptable en el caso de textos cortos \cite{bayes}. Además, es posible interpretar la probabilidad que cada palabra le asigna a cada clase. En efecto, ese vector de probabilidad discreto nos permite identificar que palabras son las que más ``empujan'' hacia alguna elección en específico.


\subsection{Modelamiento no supervisado con \textit{Latent Dirichlet Allocation}}

\insertimagerightline[\label{figure:patterns}]{img/LDA.png}{0.7}{11}{Diagrama de un modelo de tópicos LDA aplicado a texto.}

La modelización de tópicos (\textit{topic modelling} en ingles) se refiere a un tipo de modelado estadístico en el procesamiento del lenguaje natural que tiene como objetivo extraer la estructura semántica oculta de un texto. La suposición subyacente es que un documento está compuesto por una mezcla de temas abstractos, y los métodos existentes infieren esos temas basándose en las palabras de cada documento. La Asignación Latente de Dirichlet (LDA, por sus siglas en inglés) es el modelo de temas más ampliamente utilizado. Los documentos se representan como mezclas aleatorias de temas latentes. De manera similar, los temas se caracterizan por una distribución sobre todas las palabras.

\newp Dado que el número de temas a considerar en LDA es un hiperparámetro que debe establecerse de antemano, utilizaremos la familia de métricas "coherencia", que se suele usar para evaluar la calidad e interpretabilidad de los temas generados por un modelo de temas. Esta familia de métricas utiliza las relaciones entre palabras dentro de cada tópico y está diseñada para capturar la similitud semántica entre palabras dentro de un tema, con el objetivo de identificar temas que contengan palabras que tienden a frecuentemente al mismo tiempo en documentos. Los temas con una mayor coherencia se consideran más interpretables y más propensos a representar un concepto concreto.

% Since the number of topics to consider in LDA is a hyperparameter that needs to be set in advance. We will hence use the ``coherence'' family of metrics, which is often used to evaluate the quality and interpretability of the topics generated by a topic model. Reportedly, it uses the relationships between words within each topic and it is designed to capture the semantic similarity between words within a topic, aiming to identify topics that contain words that tend to co-occur frequently in real-world documents. Topics with greater coherence are considered more interpretable and more likely to represent a concrete concept.

\newp De las diversas métricas de coherencia comúnmente utilizadas en el modelado de temas, usaremos la coherencia $C_v$. Esta métrica calcula la coherencia basada en la co-ocurrencia de palabras dentro de un tema, y busca medir en qué medida las palabras en un tema tienden a aparecer juntas en el mismo contexto. Para más detalles sobre cómo se calcula esta métrica, nos referimos a \textit{Syed \& Spruit (2017)} \cite{coherence}.

% Out of the several coherence metrics commonly used in topic modeling, we will use the $C_v$ coherence. This metric calculates the coherence based on the co-occurrence of words within a topic. It aims to measure the extent to which words in a topic tend to appear together in the same context. We refer to \textit{Syed and Spruit (2017)} for the details on how such metric is computed \cite{coherence}.

\newp Para poder visualizar los tópicos de manera adecuada se usarán los siguientes conceptos vistos en el curso:
\begin{itemize}
    \item \textbf{Teoría de la información}: Se utiliza la divergencia de Jensen-Shanno, que es una métrica de disimilitud entre distribuciones de probabilidad. Dadas dos distribuciones de probabilidad, $p$ y $q$, la divergencia de Jensen-Shannon se define de la siguiente manera:
    $$ \frac{1}{2}(KL(p\|m) + KL(q \| m) ) \,,$$
    donde $m = \frac{1}{2}(p + q)$ es la distribución promedio y $KL$ denota la divergencia de Kullback-Leibler.
    \item Reducción de dimensionalidad: se utiliza el método \textit{multidimensional scalling}, que puede traducirse como ``escalamiento multidimensional''. Esta es una reducción de dimensionalidad no lineal en la cual la proyección encontrada pretende que las distancias en el nuevo espacio (en este caso de dos dimensiones) difiera lo menos posible de las distancias en el espacio original.
\end{itemize}

% The Jensen-Shannon divergence (JSD) is a measure of similarity or divergence between probability distributions. It is based on the Kullback-Leibler divergence (KL divergence) and is commonly used in information theory and statistics. Given two probability distributions, P and Q, the Jensen-Shannon divergence (JSD) is defined as follows:
$$ \frac{1}{2}(KL(p\|m) + KL(q \| m) ) $$
% where D(P || Q) represents the Kullback-Leibler divergence from distribution P to distribution Q and M is the average of the two distributions and is defined as M = 0.5 * (P + Q).
% The JSD measures the similarity between two distributions by first computing the KL divergence between each distribution and the average distribution (M), and then taking the average of these two KL divergences. The JSD ranges between 0 and 1, where 0 indicates that the two distributions are identical, and 1 indicates that they are completely dissimilar.

Esto en base a la metodología de visualización de tópicos que se propone en \textit{Sievert \& Shirley (2014)} \cite{viz}. El algoritmo, llámese como LDAvis, presenta una vista global del modelo de temas que responde preguntas sobre la prevalencia de cada tema y cómo se relacionan entre sí. En esta visualziación, los temas se representan como círculos en un plano, y los centros de los círculos se determinan calculando la distancia entre los temas y utilizando \textit{multidimensional scaling} para proyectar las distancias inter-temas en dos dimensiones. La distribución de términos de un conjunto de temas se define como el promedio ponderado de las distribuciones de términos de los temas individuales en el conjunto.

% The algorithm, namely LDAvis, presents a global view of the topic model which answers questions about the prevalence of each topic and how the topics relate to each other . In this view, the topics are plotted as circles in a two-dimensional plane, with the centers of the circles determined by computing the distance between topics and using multidimensional scaling to project the inter-topic distances onto two dimensions. Each topic's overall prevalence is encoded using the area of the circle, with topics sorted in decreasing order of prevalence. The term distribution of a cluster of topics is defined as the weighted average of the term distributions of the individual topics in the cluster.  % gpt

% \subsection{Modelos de lenguajes profundos}
% https://www.overleaf.com/project/623ddd7aeb06ec285ddc6300


\section{Resultados}

\subsection{Tarea de clasificación}

La tarea de clasificación arroja las siguientes métricas para los cuatro casos estudiados, mostrados en la tabla \ref{tab:NB_results}.

\begin{table}[htbp]
\centering
\caption{Resumen de resultados de clasificación por caso}
\begin{tabular}{l|llll|llll|}
\cline{2-9}
                                     & \multicolumn{4}{c|}{\textbf{Métrica multiclase}}                                                                                          & \multicolumn{4}{c|}{\textbf{Métrica binaria}}                                                                                                 \\ \hline
\multicolumn{1}{|l|}{\textbf{caso}}  & \multicolumn{1}{l|}{\textbf{precisión}} & \multicolumn{1}{l|}{\textbf{sensib.}} & \multicolumn{1}{l|}{\textbf{f1}}   & \textbf{exactitud} & \multicolumn{1}{l|}{\textbf{precisión}} & \multicolumn{1}{l|}{\textbf{sensib.}} & \multicolumn{1}{l|}{\textbf{f1}}   & \textbf{exactitud}     \\ \hline
\multicolumn{1}{|l|}{Julieta}        & \multicolumn{1}{l|}{0.36}               & \multicolumn{1}{l|}{\textit{0.37}}    & \multicolumn{1}{l|}{0.36}          & \textit{0.37}      & \multicolumn{1}{l|}{0.70}               & \multicolumn{1}{l|}{\textit{0.70}}    & \multicolumn{1}{l|}{0.70}          & \textit{0.70}          \\ \hline
\multicolumn{1}{|l|}{\textbf{Adela}} & \multicolumn{1}{l|}{\textbf{0.43}}      & \multicolumn{1}{l|}{\textbf{0.43}}    & \multicolumn{1}{l|}{\textbf{0.43}} & \textbf{0.43}      & \multicolumn{1}{l|}{\textbf{0.79}}      & \multicolumn{1}{l|}{\textbf{0.79}}    & \multicolumn{1}{l|}{\textbf{0.79}} & \textit{\textbf{0.79}} \\ \hline
\multicolumn{1}{|l|}{Laura}          & \multicolumn{1}{l|}{0.47}               & \multicolumn{1}{l|}{\textit{0.48}}    & \multicolumn{1}{l|}{0.48}          & \textit{0.48}      & \multicolumn{1}{l|}{0.77}               & \multicolumn{1}{l|}{\textit{0.77}}    & \multicolumn{1}{l|}{0.77}          & \textit{0.77}          \\ \hline
\multicolumn{1}{|l|}{Alicia}         & \multicolumn{1}{l|}{0.44}               & \multicolumn{1}{l|}{\textit{0.45}}    & \multicolumn{1}{l|}{0.44}          & \textit{0.45}      & \multicolumn{1}{l|}{0.83}               & \multicolumn{1}{l|}{\textit{0.83}}    & \multicolumn{1}{l|}{0.83}          & \textit{0.83}          \\ \hline
\end{tabular}
\label{tab:NB_results}
\end{table}

\newp Por otro lado, mostramos el detalle de la clasificación para el caso Adela. La tabla \ref{tab:detalle_adela} muestra la tarea de clasificación fina multiclase, mientras que la versión binaria se muestra en la tabla \ref{tab:detalle_adela_bin}.


\begin{table}[htbp]
\centering
\caption{Resultados de clasificación de posturas para caso Adela}
\begin{tabular}{|l|l|l|l|l|}
\hline
\textbf{}    & \textbf{precision} & \textbf{recall} & \textbf{f1-score} & \textbf{support} \\ \hline
(1) Producir el alimento contra el déficit            & 0.40               & 0.38            & 0.39              & 157              \\ \hline
(2)            & 0.47               & 0.47            & 0.47              & 363              \\ \hline
(3)            & 0.47               & 0.54            & 0.50              & 434              \\ \hline
(4)            & 0.35               & 0.32            & 0.33              & 257              \\ \hline
(5)           & 0.41               & 0.40            & 0.41              & 173              \\ \hline
(6) Resguardar las tradiciones indígenas            & 0.37               & 0.19            & 0.25              & 57               \\ \hline
accuracy     &                    &                 & 0.43              & 1441             \\ \hline
macro avg    & 0.41               & 0.38            & 0.39              & 1441             \\ \hline
weighted avg & 0.43               & 0.43            & 0.43              & 1441             \\ \hline
\end{tabular}
\label{tab:detalle_adela}
\end{table}

\begin{table}[htbp]
\centering
\caption{Resultados de clasificación binaria de posturas para caso Adela}
\begin{tabular}{|l|l|l|l|l|}
\hline
\textbf{}    & \textbf{precision} & \textbf{recall} & \textbf{f1-score} & \textbf{support} \\ \hline
(1) Producir el alimento contra el déficit            & 0.85               & 0.83            & 0.84              & 954              \\ \hline
(6) Resguardar las tradiciones indígenas            & 0.69               & 0.71            & 0.70              & 487               \\ \hline
accuracy     &                    &                 & 0.79              & 1441             \\ \hline
macro avg    & 0.77               & 0.77            & 0.77              & 1441             \\ \hline
weighted avg & 0.79               & 0.79            & 0.79              & 1441             \\ \hline
\end{tabular}
\label{tab:detalle_adela_bin}
\end{table}

% (1) Producir el alimento contra el déficit
% (6) Resguardar las tradiciones indígenas

\newp Para entender la naturaleza de la complejidad de la clasificación fina, se muestra la matriz de confusión en la figura \ref{img:cm_nb}. Se aprecia como hay una ligera tendencia a etiquetar texto como de clase (2), que es aquella mayoritaria. Esto es un problema recurrente en cualquier tipo de modelo, pero lo es con mayor razón en un método Naive-Bayes, pues la predicción consiste en el voto de las palabras presentes, que naturalmente aparecerán más en aquellas clases recurrrentes. 

\newp Cabe señalar que este efecto se reduzco drásticamente respecto a la versión mostrada en la presentación de avance. En esta, la matriz de confusión estaba radicalmente cargada hacia la clase mayoritaria, confundiendo cualquier clase con la clase (2). En esta versión la confusión ocurre más fuertemente con las clases vecinas (1) y (3), que corresponden a grados de convencimiento de la misma postura. Esto grafica la dificultad de la clasificación fina respecto a la versión binaria. La mejora de esta situación y de las métricas de la clasificación multiclase en general se le atribuye al procesamiento adecuado realizado al texto, no realizado en una primera instancia (incluyendo la elimación de ruido, separación de casos en Adela y eliminación de puntuación). A modo de referencia, la sensibilidad de la clase (2) antes de aplicar la metodología completa fue de 86\%, contra menos del 6\% de las clases (1), (2) y (6). Se conjetura que realizar un sobresampleo o subsampleo podría mejorar lo anterior aún más, pero nos conformamos con la mejora señalada en el la versión actual.

\insertimage[\label{img:cm_nb}]{img/adela/nb_adela_cm}{scale=0.75}{Matriz de confusión de clasificación caso Adela.}


\subsection{Interpretabilidad de modelos}

\begin{table}[htbp]
\centering
\caption{Palabras con más probabilidad por cada clase}
\begin{tabular}{l|ll|ll|}
\cline{2-5}
                                & \multicolumn{2}{l|}{\textbf{Producir el alimento}}                        & \multicolumn{2}{l|}{\textbf{Resguardar tradiciones}}                          \\ \hline
\multicolumn{1}{|l|}{\textbf{}} & \multicolumn{1}{l|}{\textbf{Palabra}}             & \textbf{Probabilidad} & \multicolumn{1}{l|}{\textbf{Palabra}}                 & \textbf{Probabilidad} \\ \hline
\multicolumn{1}{|l|}{1}         & \multicolumn{1}{l|}{\textit{\textbf{salvar}}}     & 0.9850757167193909    & \multicolumn{1}{l|}{\textit{\textbf{siglos}}}         & 0.8807510221140485    \\ \hline
\multicolumn{1}{|l|}{2}         & \multicolumn{1}{l|}{\textit{\textbf{juego}}}      & 0.9589416426994265    & \multicolumn{1}{l|}{\textit{\textbf{suplementos}}}    & 0.8630617786067957    \\ \hline
\multicolumn{1}{|l|}{3}         & \multicolumn{1}{l|}{\textit{\textbf{vidas}}}      & 0.9525060938146097    & \multicolumn{1}{l|}{\textit{\textbf{sol}}}            & 0.8253861418636597    \\ \hline
\multicolumn{1}{|l|}{4}         & \multicolumn{1}{l|}{\textit{\textbf{tribu}}}      & 0.9507237155401059    & \multicolumn{1}{l|}{\textit{\textbf{existen}}}        & 0.8227740010872212    \\ \hline
\multicolumn{1}{|l|}{5}         & \multicolumn{1}{l|}{\textit{\textbf{religiosas}}} & 0.9481281470801085    & \multicolumn{1}{l|}{\textit{\textbf{única}}}          & 0.7700217160507917    \\ \hline
\multicolumn{1}{|l|}{6}         & \multicolumn{1}{l|}{\textit{\textbf{poniendo}}}   & 0.9420201060316521    & \multicolumn{1}{l|}{\textit{\textbf{usuario}}}        & 0.761934254557929     \\ \hline
\multicolumn{1}{|l|}{7}         & \multicolumn{1}{l|}{\textit{\textbf{riesgo}}}     & 0.9383929112652638    & \multicolumn{1}{l|}{\textit{\textbf{consentimiento}}} & 0.761934254557929     \\ \hline
\multicolumn{1}{|l|}{8}         & \multicolumn{1}{l|}{\textit{\textbf{tiempos}}}    & 0.9342815991664724    & \multicolumn{1}{l|}{\textit{\textbf{integridad}}}     & 0.7303224790335253    \\ \hline
\multicolumn{1}{|l|}{9}         & \multicolumn{1}{l|}{\textit{\textbf{cambian}}}    & 0.9342815991664724    & \multicolumn{1}{l|}{\textit{\textbf{sacar}}}          & 0.7242191780159098    \\ \hline
\multicolumn{1}{|l|}{10}        & \multicolumn{1}{l|}{\textit{\textbf{ayudaria}}}   & 0.9342815991664724    & \multicolumn{1}{l|}{\textit{\textbf{obtener}}}        & 0.723129422679366     \\ \hline
\end{tabular}
\label{tab:prob_words_NB}
\end{table}


\subsection{Modelamiento de tópicos}

Se obtuvieron los siguientes números de tópicos en cada case estudiado:
\begin{itemize}
    \item Julieta: 2 tópicos
    \item Adela: 4 tópicos
    \item Laura: 4 tópicos
    \item Alicia: 4 tópicos
\end{itemize}
En la tabla \ref{tab:topicos} reportamos tanto la magnitud de cada tópico como los principales tokens que lo componen para el caso Adela.

\newp Por otro lado en la figura \ref{img:lda_viz} mostramos una visualización de los tópicos obtenidos utilizando la biblioteca de python \textit{pyLDAvis} \cite{viz}. Esta nos muestra un grado relativamente alto de sobreposición entre los tópicos 1 y 2, que son los más numerosos. Esto se condice con la similitud de los términos utilizados, poniendo en cuestión la optimalidad del número de tópicos encontrado con la métrica de coherencia. Se destaca el tópico número dos, con lo cual a la dereche de la figura se pueden ver la frecuencia relativa de las palabras más importantes de tal tema encontrado.

\begin{table}[htbp]
\centering
\caption{Términos más importantes por tópico.}
    \begin{tabular}{l|llll|}
        \cline{2-5}
                                                           & \multicolumn{4}{c|}{\textbf{Tópico (porcentaje de palabras)}}                                                                                                                   \\ \hline
        \multicolumn{1}{|l|}{Prioridad} & \multicolumn{1}{l|}{\textbf{Tópico 1 (42,4\%)}} & \multicolumn{1}{l|}{\textbf{Tópico 2 (33,5\%)}} & \multicolumn{1}{l|}{\textbf{Tópico 3 (14,9\%)}} & \textbf{Tópico 4 (9,2\%)} \\ \hline
        \multicolumn{1}{|l|}{1}                            & \multicolumn{1}{l|}{\textit{tradiciones}}       & \multicolumn{1}{l|}{tradiciones}                & \multicolumn{1}{l|}{\textit{tradiciones}}       & pueblo                    \\ \hline
        \multicolumn{1}{|l|}{2}                            & \multicolumn{1}{l|}{\textit{alimento}}          & \multicolumn{1}{l|}{alimento}                   & \multicolumn{1}{l|}{\textit{fruto}}             & si                        \\ \hline
        \multicolumn{1}{|l|}{3}                            & \multicolumn{1}{l|}{\textit{pueblo}}            & \multicolumn{1}{l|}{personas}                   & \multicolumn{1}{l|}{\textit{pueblo}}            & originario                \\ \hline
        \multicolumn{1}{|l|}{4}                            & \multicolumn{1}{l|}{\textit{llegar}}            & \multicolumn{1}{l|}{salud}                      & \multicolumn{1}{l|}{\textit{fruta}}             & producción                \\ \hline
        \multicolumn{1}{|l|}{5}                            & \multicolumn{1}{l|}{\textit{importante}}        & \multicolumn{1}{l|}{importante}                 & \multicolumn{1}{l|}{\textit{si}}                & acuerdo                   \\ \hline
        \multicolumn{1}{|l|}{6}                            & \multicolumn{1}{l|}{\textit{acuerdo}}           & \multicolumn{1}{l|}{pueblo}                     & \multicolumn{1}{l|}{\textit{pueblos}}           & producto                  \\ \hline
        \multicolumn{1}{|l|}{7}                            & \multicolumn{1}{l|}{\textit{llevar}}            & \multicolumn{1}{l|}{producir}                   & \multicolumn{1}{l|}{\textit{ser}}               & bien                      \\ \hline
        \multicolumn{1}{|l|}{8}                            & \multicolumn{1}{l|}{\textit{producción}}        & \multicolumn{1}{l|}{niños}                      & \multicolumn{1}{l|}{\textit{proyecto}}          & alimento                  \\ \hline
        \multicolumn{1}{|l|}{9}                            & \multicolumn{1}{l|}{\textit{originario}}        & \multicolumn{1}{l|}{priorizar}                  & \multicolumn{1}{l|}{\textit{originarios}}       & puede                     \\ \hline
        \multicolumn{1}{|l|}{10}                           & \multicolumn{1}{l|}{\textit{Producir}}          & \multicolumn{1}{l|}{puede}                      & \multicolumn{1}{l|}{\textit{manera}}            & tradición                 \\ \hline
        \end{tabular}
    \label{tab:topicos}
\end{table}

\insertimage[\label{img:lda_viz}]{img/adela/LDA_viz}{scale=0.4}{Visualización de tópicos ajustados con LDA.}


\section{Trabajo Futuro}

Lamentablemente no se abordaron la gran variedad de técnicas a disposición para enfrentar este proyecto. Además, la naturaleza del \textit{dataset} permitía plantear distintas tareas de predicción, cosa que se mencionó en las presentaciones del curso.

\begin{enumerate}
    \item Ampliar el respectro de modelos utilizados tanto para clasificación como para modelamiento de tópicos, incluyendo modelos profundos. Comparar la capacidad de predicción tanto con los algoritmos básicos como con la capacidad humana. En particular, se mencionan algunos métodos que no se alcanzaron a abordar en este trabajo:
    \begin{itemize}
        \item Topic Modelling con \textit{BERT Topic}
        \item Modelos basados en árboles usando vectores basados en la ocurrencia de los distintos tópicos.
        \item Utilización de modelos de lenguaje para la vectorización de las distintas respuestas de texto.
    \end{itemize}
    \item Implementar los modelos utilizados en este trabajo pero para la predicción del cambio de respuesta de una etapa a otra, usando las justificaciones de la etapa intermedia y última etapa. Además, se pretende identificar elementos semánticos en cambios de valoraciones entre distintas etapas de la actividad, tanto con elementos diferentes como comunes entre ambas justificaciones.
    \insertimage[\label{img:modelo2}]{img/modelo2}{scale=0.75}{Ejemplo de modelo para predecir cambios de postura.}
    \item Utilizar modelos predictivos de texto para predecir el grado de competencia ética en las justificaciones, utilizando tanto técnicas simples como avanzadas de procesamiento de lenguaje natural. Incluir modelos de reconocimiento de entidades para la identificación automática de elementos textuales que denoten elementos positivos y negativos en cuanto a la calidad de la respuesta otorgada. Notemos sin embargo que estas tareas requieren la creación de un dataset con etiquetas especiales.

    \insertimage[\label{img:modelo3}]{img/modelo3}{scale=0.7}{Ejemplo de modelo para asistencia a la evaluación de competencia ética.}
\end{enumerate}


\section{Conclusiones}

Los modelos usados en este trabajo fueron modelos simples basados en conteos de palabras, pero cuya formulación probabilística los dota de la capacidad de clasificar nuevos textos, tanto de manera supervisada como no supervisada.  Estas técnicas son además interpretables. En el caso específico de LDA y Naive-Bayes, tenemos tanto una distribución de palabras por clase como una distribución de clase por palabra, lo cual es particularmente útil para el equipo de la unidad de ética tome conclusiones respecto al lenguaje empleado en las decisiones de estudiantes. Esto marca un contraste con los métodos de aprendizaje profundo. Estos han mostrado una gran capacidad predictiva, pero esta capacidad es inútil si en este caso particular, pues la predicción como tal no es de interés directo de la unidad de ética.

\newp Por lo demás, el desarrollo de un método de aprendizaje profundo que si sea capaz de declarar aquellas variables (deseablemente, conceptos) que sean destacables a la hora de reflejar la competencia ética, es un trabajo futuro a considerar. Existe una buena perspectiva de que funcione dada la naturaleza de los textos que han mostrado tanto el análisis exploratorio como los modelos utilizados y sus métricas de desempeño. En particular modelos profundos podrían captar situaciones semánticas que modelos basados en conteo no logran reflejar, como lo son las negaciones. 

\newp En cualquier caso, el presente trabajo muestra una metodología con la capacidad de dar una fotografía global de la naturaleza de las respuestas y los conceptos empleados para justificarla. Se espera que esta herramienta ayude a graficar la coherencia de las respuestas cada año, con lo cual se pueden tomar decisiones informadas y justificadas respecto a la metodología para la enseñanza de la ética en la facultad de ciencias físicas y matemáticas.

\iffalse
\begin{images}[\label{figure:glove}]{Descripción general}
    \addimage{img1}{width=5.5cm}{}
    \addimage{img2}{width=5.3cm}{}
    \imagesnewline
    \addimage{img3}{width=9cm}{}
\end{images}
\fi


% -------------------------------------------------------------------------------------------

\begin{references}

\small

    \bibitem{garten} Garten, J., Boghrati, R., Hoover, J., Johnson, K. M., & Dehghani, M. (2016). Morality Between the Lines: Detecting Moral Sentiment In Text. Proceedings of IJCAI 2016 Workshop on Computational Modeling of Attitudes.

    \bibitem{tfd} Graham, J., Haidt, J., & Nosek, B. A. (2009). Liberals and conservatives rely on different sets of moral foundations. Journal of Personality and Social Psychology, 96, 1029–1046. \url{https://doi.org/10.1037/a0015141}
    
    \bibitem{tmf} Haidt, J. (2007). The New Synthesis in Moral Psychology. Science, 316(5827), 998–1002. \url{https://doi.org/10.1126/science.1137651}
    
    \bibitem{xie} Xie, J. Y., Hirst, G., & Xu, Y. (2020). Contextualized moral inference (arXiv:2008.10762). arXiv. \url{https://doi.org/10.48550/arXiv.2008.10762}
    
    \bibitem{diff} Kennedy, B., Atari, M., Mostafazadeh Davani, A., Hoover, J., Omrani, A., Graham, J., & Dehghani, M. (2021). Moral concerns are differentially observable in language. Cognition, 212, 104696. \url{https://doi.org/10.1016/j.cognition.2021.104696}
    
    \bibitem{lda} Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. The Journal of Machine Learning Research, 3(null), 993–1022.
    
    \bibitem{bert} Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 4171–4186. \url{https://doi.org/10.18653/v1/N19-1423}
    
    \bibitem{bayes}
        Metsis, V., Androutsopoulos, I.,  Paliouras, G. (2006, July). Spam filtering with naive bayes-which naive bayes?. In CEAS(Vol. 17, pp. 28-69)
    
    \bibitem{coherence} Syed, S., & Spruit, M. (2017, October). Full-text or abstract? examining topic coherence scores using latent dirichlet allocation. In 2017 IEEE International conference on data science and advanced analytics (DSAA) (pp. 165-174). IEEE. \url{https://doi.org/10.1109/DSAA.2017.61}
    
    \bibitem{viz} Sievert, C., & Shirley, K. (2014, June). LDAvis: A method for visualizing and interpreting topics. In Proceedings of the workshop on interactive language learning, visualization, and interfaces (pp. 63-70). \url{https://github.com/bmabey/pyLDAvis}
    
    
\end{references}



% FIN DEL DOCUMENTO
\end{document}
