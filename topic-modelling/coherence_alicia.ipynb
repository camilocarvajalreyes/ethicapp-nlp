{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Número óptimo de Tópicos con métrica Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 rows found with non string elements for column comment (0.65%)\n",
      "Deleting 2326 columns for which max target value is over 7 (18.76%)\n",
      "9991 available rows after processing\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from utils.cargar import df_caso\n",
    "from utils.preprocesamiento import process_df, StemmerTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tokenizador = StemmerTokenizer(stem=False,rmv_punctuation=True)\n",
    "\n",
    "caso = 'alicia'\n",
    "\n",
    "df = df_caso(caso)\n",
    "df = process_df(df,'comment','sel',verbose=True)\n",
    "df = df.drop(columns=['user_id','team_id','gender','df','title','opt_left','opt_right','max_num','phase','time','curso'])\n",
    "\n",
    "df_train, df_test, _, _ = train_test_split(df, df['sel'], test_size=.05, stratify=df['sel'], random_state=0)\n",
    "tokenized_corpus = [tokenizador(document) for document in df_train['comment']]\n",
    "tokenized_test = [tokenizador(document) for document in df_test['comment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Create a dictionary from the tokenized corpus\n",
    "dictionary = corpora.Dictionary(tokenized_corpus)\n",
    "\n",
    "# Convert the tokenized corpus into a document-term matrix\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in tokenized_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best number of topics: 6\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Set the range of topic numbers to try\n",
    "min_topics = 2\n",
    "max_topics = 10\n",
    "step_size = 1\n",
    "\n",
    "# Initialize variables for best coherence score and best number of topics\n",
    "best_coherence_score = -1\n",
    "best_num_topics = -1\n",
    "\n",
    "# Iterate over the range of topic numbers\n",
    "for num_topics in range(min_topics, max_topics+1, step_size):\n",
    "    # Train the LDA model\n",
    "    lda_model = gensim.models.LdaModel(doc_term_matrix, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "    \n",
    "    # Calculate coherence score\n",
    "    coherence_model = CoherenceModel(model=lda_model, texts=tokenized_corpus, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    \n",
    "    # Check if coherence score is the best so far\n",
    "    if coherence_score > best_coherence_score:\n",
    "        best_coherence_score = coherence_score\n",
    "        best_num_topics = num_topics\n",
    "\n",
    "# Print the best number of topics\n",
    "print(f\"Best number of topics: {best_num_topics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.3 s, sys: 3.95 ms, total: 25.3 s\n",
      "Wall time: 25.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda_model_opt = gensim.models.LdaModel(doc_term_matrix, num_topics=best_num_topics, id2word=dictionary, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.043*\"reputación\" + 0.030*\"transparencia\" + 0.028*\"ser\" + 0.023*\"usuarios\" + 0.020*\"si\" + 0.019*\"transparente\" + 0.018*\"mantener\" + 0.015*\"mandante\" + 0.013*\"falencias\" + 0.013*\"profesional\"')\n",
      "(1, '0.028*\"entregar\" + 0.025*\"producto\" + 0.022*\"mejor\" + 0.019*\"calidad\" + 0.018*\"proyecto\" + 0.018*\"importante\" + 0.017*\"tiempo\" + 0.015*\"plazo\" + 0.014*\"si\" + 0.013*\"bien\"')\n",
      "(2, '0.038*\"contexto\" + 0.030*\"acuerdos\" + 0.025*\"condiciones\" + 0.021*\"cumplir\" + 0.020*\"Alicia\" + 0.018*\"pandemia\" + 0.017*\"proyecto\" + 0.015*\"debe\" + 0.012*\"situación\" + 0.012*\"mundial\"')\n",
      "(3, '0.018*\"debe\" + 0.012*\"proyecto\" + 0.011*\"cumplir\" + 0.009*\"plazos\" + 0.008*\"importante\" + 0.008*\"postura\" + 0.007*\"plazo\" + 0.006*\"Alicia\" + 0.005*\"empresa\" + 0.005*\"bien\"')\n",
      "(4, '0.040*\"proyecto\" + 0.019*\"si\" + 0.019*\"puede\" + 0.016*\"trabajo\" + 0.014*\"bien\" + 0.013*\"entregar\" + 0.011*\"ser\" + 0.011*\"consecuencias\" + 0.010*\"plazo\" + 0.010*\"podría\"')\n",
      "(5, '0.066*\"proyecto\" + 0.037*\"criterios\" + 0.030*\"técnicos\" + 0.025*\"priorizar\" + 0.025*\"plazos\" + 0.025*\"usuarios\" + 0.018*\"debe\" + 0.017*\"cumplir\" + 0.016*\"Alicia\" + 0.015*\"usuario\"')\n"
     ]
    }
   ],
   "source": [
    "# Print the generated topics\n",
    "topics = lda_model_opt.print_topics(num_topics=num_topics)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Es importante producir producto aceptable incluso si demora pues hacer producto mal puede llevar repercuciones grandes\n",
      "\n",
      "Topic 0: 0.08216912299394608\n",
      "Topic 1: 0.2802342474460602\n",
      "Topic 4: 0.6077150702476501\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(tokenized_test[0]) + '\\n')\n",
    "\n",
    "# Convert the tokenized document into a document-term matrix\n",
    "doc_term_matrix = [dictionary.doc2bow(tokenized_test[0])]\n",
    "\n",
    "# Get the topic probabilities for the new document\n",
    "topic_probs = lda_model_opt.get_document_topics(doc_term_matrix)[0]\n",
    "\n",
    "# Print the topic probabilities\n",
    "for topic, prob in topic_probs:\n",
    "    print(f\"Topic {topic}: {prob}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ethics_env",
   "language": "python",
   "name": "ethics_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
