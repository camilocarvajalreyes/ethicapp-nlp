{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Número óptimo de Tópicos con métrica Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 rows found with non string elements for column comment (0.67%)\n",
      "Deleting 255 columns for which max target value is over 7 (5.00%)\n",
      "4806 available rows after processing\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from utils.cargar import df_caso\n",
    "from utils.preprocesamiento import process_df, StemmerTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tokenizador = StemmerTokenizer(stem=False,rmv_punctuation=True)\n",
    "\n",
    "caso = 'julieta'\n",
    "\n",
    "df = df_caso(caso)\n",
    "df = process_df(df,'comment','sel',verbose=True)\n",
    "df = df.drop(columns=['user_id','team_id','gender','df','title','opt_left','opt_right','max_num','phase','time','curso'])\n",
    "\n",
    "df_train, df_test, _, _ = train_test_split(df, df['sel'], test_size=.05, stratify=df['sel'], random_state=0)\n",
    "tokenized_corpus = [tokenizador(document) for document in df_train['comment']]\n",
    "tokenized_test = [tokenizador(document) for document in df_test['comment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Create a dictionary from the tokenized corpus\n",
    "dictionary = corpora.Dictionary(tokenized_corpus)\n",
    "\n",
    "# Convert the tokenized corpus into a document-term matrix\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in tokenized_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best number of topics: 2\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Set the range of topic numbers to try\n",
    "min_topics = 2\n",
    "max_topics = 10\n",
    "step_size = 1\n",
    "\n",
    "# Initialize variables for best coherence score and best number of topics\n",
    "best_coherence_score = -1\n",
    "best_num_topics = -1\n",
    "\n",
    "# Iterate over the range of topic numbers\n",
    "for num_topics in range(min_topics, max_topics+1, step_size):\n",
    "    # Train the LDA model\n",
    "    lda_model = gensim.models.LdaModel(doc_term_matrix, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "    \n",
    "    # Calculate coherence score\n",
    "    coherence_model = CoherenceModel(model=lda_model, texts=tokenized_corpus, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    \n",
    "    # Check if coherence score is the best so far\n",
    "    if coherence_score > best_coherence_score:\n",
    "        best_coherence_score = coherence_score\n",
    "        best_num_topics = num_topics\n",
    "\n",
    "# Print the best number of topics\n",
    "print(f\"Best number of topics: {best_num_topics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.2 s, sys: 28 ms, total: 29.2 s\n",
      "Wall time: 29.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda_model_opt = gensim.models.LdaModel(doc_term_matrix, num_topics=best_num_topics, id2word=dictionary, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.035*\"información\" + 0.020*\"si\" + 0.019*\"Julieta\" + 0.019*\"beca\" + 0.017*\"usar\" + 0.014*\"debería\" + 0.012*\"podría\" + 0.011*\"futuro\" + 0.009*\"puede\" + 0.008*\"grupo\"')\n",
      "(1, '0.033*\"si\" + 0.021*\"beca\" + 0.019*\"copiar\" + 0.014*\"respuesta\" + 0.014*\"debería\" + 0.014*\"Julieta\" + 0.011*\"puede\" + 0.010*\"control\" + 0.009*\"caso\" + 0.007*\"nota\"')\n"
     ]
    }
   ],
   "source": [
    "# Print the generated topics\n",
    "topics = lda_model_opt.print_topics(num_topics=num_topics)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No deberia utilizarla forma obtendra nota probable refleje aprendizaje perjudicara hora aprender contenidos futuros\n",
      "\n",
      "Topic 0: 0.7497064471244812\n",
      "Topic 1: 0.2502935230731964\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(tokenized_test[0]) + '\\n')\n",
    "\n",
    "# Convert the tokenized document into a document-term matrix\n",
    "doc_term_matrix = [dictionary.doc2bow(tokenized_test[0])]\n",
    "\n",
    "# Get the topic probabilities for the new document\n",
    "topic_probs = lda_model_opt.get_document_topics(doc_term_matrix)[0]\n",
    "\n",
    "# Print the topic probabilities\n",
    "for topic, prob in topic_probs:\n",
    "    print(f\"Topic {topic}: {prob}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ethics_env",
   "language": "python",
   "name": "ethics_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
